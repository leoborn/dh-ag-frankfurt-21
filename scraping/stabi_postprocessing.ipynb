{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104da309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# With this, we can use regular expressions.\n",
    "import re\n",
    "# We import this sub-class for easy generation of frequency distributions.\n",
    "from nltk import FreqDist\n",
    "# This library is for normalizing dates.\n",
    "import dateparser\n",
    "# This is the NLP toolkit we use.\n",
    "import spacy\n",
    "# We initialize a German NLP pipeline with the medium-sized language model.\n",
    "nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"japanese_students.json\") as json_file:\n",
    "  data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of students:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f2fa4",
   "metadata": {},
   "source": [
    "# Student name processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf73b9",
   "metadata": {},
   "source": [
    "## Task: Separate Japanese from Romaji names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd365198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a regular expression for a sequence of unicode characters in the CJK range.\n",
    "JAPANESE_CHARACTERS_PATTERN = r'[\\u4e00-\\u9fff]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for student in data:\n",
    "  print(student)\n",
    "  name = student['Name']\n",
    "  jap_characters_found = re.findall(JAPANESE_CHARACTERS_PATTERN, name)\n",
    "  print(jap_characters_found)\n",
    "  # This means that no Japanese characters appear in the name.\n",
    "  if len(jap_characters_found) == 0:\n",
    "    print(\"No Japanese name!\")\n",
    "    print(\"Romaji name:\", name)\n",
    "  else:\n",
    "    # Japanese name is the last element of the String split at whitespaces.\n",
    "    name_jpn = name.split(\" \")[-1]\n",
    "    print(\"Japanese Name:\", name_jpn)\n",
    "    # For the Romaji name, we take all elements except the last of the String split at whitespaces.\n",
    "    # For example, this might result in [\"Abe\", \"Isoo\"].\n",
    "    # We then join this List to a String using a whitespace -> \"Abe Isoo\".\n",
    "    name_romaji = \" \".join(name.split(\" \")[:-1])\n",
    "    print(\"Romaji Name:\", name_romaji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ce390",
   "metadata": {},
   "source": [
    "# Student date processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f8dc2",
   "metadata": {},
   "source": [
    "## Task: Parse all given dates and get all birth years and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might encounter ill-formed dates, so we set a counter to see how many dates could not be parsed.\n",
    "unparseable_dates = 0\n",
    "# We initialize an empty List to collect all parsed birth years.\n",
    "birth_year_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for student in data:\n",
    "  # The date string might contain unnecessary whitespaces or line breaks, so we remove them.\n",
    "  dates = student['Daten'].strip()\n",
    "    \n",
    "  # Dates are given in the format date1–date2.\n",
    "  print(\"Birth and death dates:\", dates)\n",
    "    \n",
    "  # We separate them by splitting at \"–\".\n",
    "  date_list = dates.split(\"–\")\n",
    "\n",
    "  # We are only interested in dates that are not just \"-\".\n",
    "  #\n",
    "  # Examples:\n",
    "  # A date \"-1913\" would result in [\"\",\"1913\"],\n",
    "  # a date \"Juni 1880-\" would result in [\"Juni 1880\",\"\"],\n",
    "  # and a date \"3.4.1857-Dezember 1910\" would result in [\"3.4.1857\", \"Dezember 1910\"].\n",
    "  if len(date_list) == 2:\n",
    "    first_date = date_list[0]\n",
    "    second_date = date_list[1]\n",
    "    # Only continue if non-empty.\n",
    "    if first_date != \"\":\n",
    "      try:\n",
    "        birth_date = dateparser.parse(first_date)\n",
    "        print(\"Birth date:\", birth_date)\n",
    "        # The year of a parsed date can be accessed with .year.\n",
    "        print(\"Birth year:\", birth_date.year)\n",
    "        birth_year_list.append(birth_date.year)\n",
    "      except:\n",
    "        print(first_date, \"cannnot be parsed!\")\n",
    "        unparseable_dates += 1\n",
    "    # Only continue if non-empty.\n",
    "    if second_date != \"\":\n",
    "      try:\n",
    "        death_date = dateparser.parse(second_date)\n",
    "        print(\"Death date:\", death_date)\n",
    "        print(\"Death year:\", death_date.year)\n",
    "      except:\n",
    "        print(second_date, \"cannnot be parsed!\")\n",
    "        unparseable_dates += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unparseable_dates, \"dates could not be parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b967dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically generate a frequency distribution from the birth year List.\n",
    "fdist = FreqDist(birth_year_list)\n",
    "# Print out the 20 most common birth years.\n",
    "print(\"Most common birth years and their frequencies:\")\n",
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bbf3a",
   "metadata": {},
   "source": [
    "## Exercise: Get the death year distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8089637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize an empty List to collect all parsed death years.\n",
    "death_year_list = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ca18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for student in data:\n",
    "  dates = student['Daten'].strip()\n",
    "  date_list = \n",
    "  # We are only interested in dates that are not just \"-\".\n",
    "  #\n",
    "  # Examples:\n",
    "  # A date \"-1913\" would result in [\"\",\"1913\"],\n",
    "  # a date \"Juni 1880-\" would result in [\"Juni 1880\",\"\"],\n",
    "  # and a date \"3.4.1857-Dezember 1910\" would result in [\"3.4.1857\", \"Dezember 1910\"].\n",
    "  if len(date_list) == 2:\n",
    "    second_date = date_list[1]\n",
    "    # MORE CODE COMES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91604642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically generate a frequency distribution from the birth year List.\n",
    "fdist = \n",
    "# Print out the 15 most common death years.\n",
    "print(\"Most common death years and their frequencies:\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b441b",
   "metadata": {},
   "source": [
    "# Student text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532f8a0",
   "metadata": {},
   "source": [
    "## Preliminary: Using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose, we have a sentence.\n",
    "sentence = \"Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spacy to parse a text is as simple as invoking nlp(text).\n",
    "parsed_sentence = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default element when iterating over a parsed text is a token.\n",
    "for token in parsed_sentence:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose, we have more than one sentence.\n",
    "text = \"Jemand musste Josef K. verleumdet haben, denn ohne daß er etwas Böses getan hätte \" +\\\n",
    "\"wurde er eines Morgens verhaftet. Die Köchin der Frau Grubach, seiner Zimmervermieterin \" +\\\n",
    "\"die ihm jeden Tag gegen acht Uhr früh das Frühstück brachte, kam diesmal nicht. \" +\\\n",
    "\"Das war noch niemals geschehen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eaccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can automatically segment the text into sentences with the .sents method.\n",
    "for index,sent in enumerate(parsed_text.sents):\n",
    "    print(\"Sentence\", index,\":\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entities (e.g. persons, places, or companies) can be accessed using the .ents method.\n",
    "for named_entity in parsed_text.ents:\n",
    "    print(named_entity.text, named_entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not only are named entities accessible over the whole text, but also for individual sentences.\n",
    "for index,sent in enumerate(parsed_text.sents):\n",
    "    for named_entity in sent.ents:\n",
    "        print(\"Entity in sentence\",index,\":\", named_entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c434139",
   "metadata": {},
   "source": [
    "## Exercise: Be creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think of a text than contains persons, places, companies etc.\n",
    "some_text = \"\"\n",
    "parsed = nlp(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all named entities (persons, places, companies etc.) from your text and display their labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fce865",
   "metadata": {},
   "source": [
    "## Task 1: Retrieve all visited universities and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with exploring the first five entries.\n",
    "for student in data[:5]:\n",
    "  text = student['Text']\n",
    "  print(\"Student text:\", text)\n",
    "  # Running spacy on the text amounts to calling nlp(text).\n",
    "  parsed_text = nlp(text)\n",
    "  # We can access all named entities using .ents.\n",
    "  for named_entity in parsed_text.ents:\n",
    "    # Simply print each entity's text and label.\n",
    "    print(named_entity.text, named_entity.label_)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e833a",
   "metadata": {},
   "source": [
    "### What do you notice about the above output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb67851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize an empty List.\n",
    "all_universities = []\n",
    "for student in data[:5]:\n",
    "  text = student['Text']\n",
    "  parsed_text = nlp(text)\n",
    "  for named_entity in parsed_text.ents:\n",
    "    # We take all named entities starting with \"U \" (e.g. \"U Berlin\") to be indicative of a university.\n",
    "    if named_entity.text.startswith(\"U \"):\n",
    "      all_universities.append(named_entity.text)\n",
    "print(\"All universities:\", all_universities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f86d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_universities = []\n",
    "# Now, we iterate over all students.\n",
    "for student in data:\n",
    "  text = student['Text']\n",
    "  parsed_text = nlp(text)\n",
    "  for named_entity in parsed_text.ents:\n",
    "    if named_entity.text.startswith(\"U \"):\n",
    "      all_universities.append(named_entity.text)\n",
    "\n",
    "# Automatically generate a frequency distribution from the university List.\n",
    "uni_fdist = FreqDist(all_universities)\n",
    "# Print out the 20 most commonly visited universities.\n",
    "print(\"Most common universities and their frequencies:\")\n",
    "print(uni_fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250a6d6",
   "metadata": {},
   "source": [
    "## Task 2: Retrieve all place names from the text commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702be946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize an empty dictionary.\n",
    "persons_to_text_places = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "for student in data:\n",
    "  name = student['Name']\n",
    "  persons_to_text_places[name] = []\n",
    "  text = student['Text']\n",
    "  # Running spacy on the text amounts to calling nlp(text).\n",
    "  parsed_text = nlp(text)\n",
    "  # We can access all named entities using .ents.\n",
    "  for named_entity in parsed_text.ents:\n",
    "    if named_entity.label_ == \"LOC\":\n",
    "      persons_to_text_places[name].append(named_entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c144eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(persons_to_text_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since some persons might not have locations found by spacy, we can remove them\n",
    "# by creating a new dictionary with the condition that the value (== place name List) is non-empty.\n",
    "filtered_persons_to_text_places = {k:v for k,v in persons_to_text_places.items() if len(v) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_persons_to_text_places)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73eb372",
   "metadata": {},
   "source": [
    "## Exercise: Retrieve all persons and organisations from the the text commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize an empty dictionary.\n",
    "persons_to_text_persons = \n",
    "# We initialize another empty dictionary.\n",
    "persons_to_text_organisations = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the result more readable, let's only focus on the first 50 persons.\n",
    "for student in data[:50]:\n",
    "    name = student['Name']\n",
    "    persons_to_text_persons[name] = []\n",
    "    persons_to_text_organisations[name] = []\n",
    "    text = student['Text']\n",
    "    # MORE CODE COMES HERE\n",
    "    parsed_text = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41868a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_persons_to_text_persons = \n",
    "print(filtered_persons_to_text_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_persons_to_text_organisations = \n",
    "print(filtered_persons_to_text_organisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4322d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
